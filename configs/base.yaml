# Base Configuration for TMRL Experiments
# This represents the current baseline system

name: baseline
description: "Baseline TMRL configuration matching current system"
reward_type: baseline
model_type: mlp
algorithm: sac

# Reward Function Parameters
reward_params:
  nb_obs_forward: 10
  nb_obs_backward: 10
  nb_zero_rew_before_failure: 10
  min_nb_steps_before_failure: 70
  max_dist_from_traj: 60.0

# Model Parameters
model_params:
  hidden_sizes: [256, 256]
  activation: relu

# Training Algorithm Parameters  
training_params:
  # SAC parameters
  lr_actor: 0.0003
  lr_critic: 0.00005
  lr_entropy: 0.0003
  gamma: 0.995
  polyak: 0.995
  alpha: 0.37
  learn_entropy_coef: false
  target_entropy: -7.0
  
  # Training loop parameters
  memory_size: 1000000
  batch_size: 256
  epochs: 10000
  rounds: 10
  steps: 1000
  update_model_interval: 100
  update_buffer_interval: 100
  max_training_steps_per_env_step: 2.0
  start_training: 400
  cuda: true

# Environment Parameters
env_params:
  interface_type: lidar
  img_hist_len: 4
  gamepad: true
  rtgym:
    time_step_duration: 0.05
    start_obs_capture: 0.04
    time_step_timeout_factor: 1.0
    act_buf_len: 2
    benchmark: false
    wait_on_done: true
    ep_max_length: 1000

tags:
  - baseline
  - mlp
  - lidar

